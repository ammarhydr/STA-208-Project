{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution2D, Flatten, Dense\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_image_data_format('channels_first')\n",
    "\n",
    "ENV_NAME = 'Breakout-v0'  # Environment name\n",
    "FRAME_WIDTH = 84  # Resized frame width\n",
    "FRAME_HEIGHT = 84  # Resized frame height\n",
    "NUM_EPISODES = 30000  # Number of episodes the agent plays\n",
    "STATE_LENGTH = 4  # Number of most recent frames to produce the input to the network\n",
    "GAMMA = 0.99  # Discount factor\n",
    "EXPLORATION_STEPS = 1000000  # Number of steps over which the initial value of epsilon is linearly annealed to its final value\n",
    "INITIAL_EPSILON = 1.0  # Initial value of epsilon in epsilon-greedy\n",
    "FINAL_EPSILON = 0.1  # Final value of epsilon in epsilon-greedy\n",
    "INITIAL_REPLAY_SIZE = 20000  # Number of steps to populate the replay memory before training starts\n",
    "NUM_REPLAY_MEMORY = 400000  # Number of replay memory the agent uses for training\n",
    "BATCH_SIZE = 32  # Mini batch size\n",
    "TARGET_UPDATE_INTERVAL = 10000  # The frequency with which the target network is updated\n",
    "TRAIN_INTERVAL = 4  # The agent selects 4 actions between successive updates\n",
    "LEARNING_RATE = 0.00025  # Learning rate used by RMSProp\n",
    "MOMENTUM = 0.95  # Momentum used by RMSProp\n",
    "MIN_GRAD = 0.01  # Constant added to the squared gradient in the denominator of the RMSProp update\n",
    "SAVE_INTERVAL = 300000  # The frequency with which the network is saved\n",
    "NO_OP_STEPS = 30  # Maximum number of \"do nothing\" actions to be performed by the agent at the start of an episode\n",
    "LOAD_NETWORK = False\n",
    "TRAIN = True\n",
    "SAVE_NETWORK_PATH = 'saved_networks/' + ENV_NAME\n",
    "SAVE_SUMMARY_PATH = 'summary/' + ENV_NAME\n",
    "NUM_EPISODES_AT_TEST = 30  # Number of episodes the agent plays at test time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, num_actions):\n",
    "        self.num_actions = num_actions\n",
    "        self.epsilon = INITIAL_EPSILON\n",
    "        self.epsilon_step = (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORATION_STEPS\n",
    "        self.t = 0\n",
    "\n",
    "        # Parameters used for summary\n",
    "        self.total_reward = 0\n",
    "        self.total_q_max = 0\n",
    "        self.total_loss = 0\n",
    "        self.duration = 0\n",
    "        self.episode = 0\n",
    "\n",
    "        # Create replay memory\n",
    "        self.replay_memory = deque()\n",
    "\n",
    "        # Create q network\n",
    "        self.s, self.q_values, q_network = self.build_network()\n",
    "        q_network_weights = q_network.trainable_weights\n",
    "\n",
    "        # Create target network\n",
    "        self.st, self.target_q_values, target_network = self.build_network()\n",
    "        target_network_weights = target_network.trainable_weights\n",
    "\n",
    "        # Define target network update operation\n",
    "        self.update_target_network = [target_network_weights[i].assign(q_network_weights[i]) for i in range(len(target_network_weights))]\n",
    "\n",
    "        # Define loss and gradient update operation\n",
    "        self.a, self.y, self.loss, self.grads_update = self.build_training_op(q_network_weights)\n",
    "\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.saver = tf.train.Saver(q_network_weights)\n",
    "        self.summary_placeholders, self.update_ops, self.summary_op = self.setup_summary()\n",
    "        self.summary_writer = tf.summary.FileWriter(SAVE_SUMMARY_PATH, self.sess.graph)\n",
    "\n",
    "        if not os.path.exists(SAVE_NETWORK_PATH):\n",
    "            os.makedirs(SAVE_NETWORK_PATH)\n",
    "\n",
    "        self.sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        # Load network\n",
    "        if LOAD_NETWORK:\n",
    "            self.load_network()\n",
    "\n",
    "        # Initialize target network\n",
    "        self.sess.run(self.update_target_network)\n",
    "\n",
    "    def build_network(self):\n",
    "        model = Sequential()\n",
    "        model.add(Convolution2D(32, 8, 8, subsample=(4, 4), activation='relu', input_shape=(STATE_LENGTH, FRAME_WIDTH, FRAME_HEIGHT)))\n",
    "        model.add(Convolution2D(64, 4, 4, subsample=(2, 2), activation='relu'))\n",
    "        model.add(Convolution2D(64, 3, 3, subsample=(1, 1), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(self.num_actions))\n",
    "\n",
    "        s = tf.placeholder(tf.float32, [None, STATE_LENGTH, FRAME_WIDTH, FRAME_HEIGHT])\n",
    "        q_values = model(s)\n",
    "\n",
    "        return s, q_values, model\n",
    "\n",
    "    def build_training_op(self, q_network_weights):\n",
    "        a = tf.placeholder(tf.int64, [None])\n",
    "        y = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "        # Convert action to one hot vector\n",
    "        a_one_hot = tf.one_hot(a, self.num_actions, 1.0, 0.0)\n",
    "        q_value = tf.reduce_sum(tf.multiply(self.q_values, a_one_hot), reduction_indices=1)\n",
    "\n",
    "        # Clip the error, the loss is quadratic when the error is in (-1, 1), and linear outside of that region\n",
    "        error = tf.abs(y - q_value)\n",
    "        quadratic_part = tf.clip_by_value(error, 0.0, 1.0)\n",
    "        linear_part = error - quadratic_part\n",
    "        loss = tf.reduce_mean(0.5 * tf.square(quadratic_part) + linear_part)\n",
    "\n",
    "        optimizer = tf.train.RMSPropOptimizer(LEARNING_RATE, momentum=MOMENTUM, epsilon=MIN_GRAD)\n",
    "        grads_update = optimizer.minimize(loss, var_list=q_network_weights)\n",
    "\n",
    "        return a, y, loss, grads_update\n",
    "\n",
    "    def get_initial_state(self, observation, last_observation):\n",
    "        processed_observation = np.maximum(observation, last_observation)\n",
    "        processed_observation = np.uint8(resize(rgb2gray(processed_observation), (FRAME_WIDTH, FRAME_HEIGHT)) * 255)\n",
    "        state = [processed_observation for _ in range(STATE_LENGTH)]\n",
    "        return np.stack(state, axis=0)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if self.epsilon >= random.random() or self.t < INITIAL_REPLAY_SIZE:\n",
    "            action = random.randrange(self.num_actions)\n",
    "        else:\n",
    "            action = np.argmax(self.q_values.eval(feed_dict={self.s: [np.float32(state / 255.0)]}))\n",
    "\n",
    "        # Anneal epsilon linearly over time\n",
    "        if self.epsilon > FINAL_EPSILON and self.t >= INITIAL_REPLAY_SIZE:\n",
    "            self.epsilon -= self.epsilon_step\n",
    "\n",
    "        return action\n",
    "\n",
    "    def run(self, state, action, reward, terminal, observation):\n",
    "        next_state = np.append(state[1:, :, :], observation, axis=0)\n",
    "\n",
    "        # Clip all positive rewards at 1 and all negative rewards at -1, leaving 0 rewards unchanged\n",
    "        reward = np.clip(reward, -1, 1)\n",
    "\n",
    "        # Store transition in replay memory\n",
    "        self.replay_memory.append((state, action, reward, next_state, terminal))\n",
    "        if len(self.replay_memory) > NUM_REPLAY_MEMORY:\n",
    "            self.replay_memory.popleft()\n",
    "\n",
    "        if self.t >= INITIAL_REPLAY_SIZE:\n",
    "            # Train network\n",
    "            if self.t % TRAIN_INTERVAL == 0:\n",
    "                self.train_network()\n",
    "\n",
    "            # Update target network\n",
    "            if self.t % TARGET_UPDATE_INTERVAL == 0:\n",
    "                self.sess.run(self.update_target_network)\n",
    "\n",
    "            # Save network\n",
    "            if self.t % SAVE_INTERVAL == 0:\n",
    "                save_path = self.saver.save(self.sess, SAVE_NETWORK_PATH + '/' + ENV_NAME, global_step=self.t)\n",
    "                print('Successfully saved: ' + save_path)\n",
    "\n",
    "        self.total_reward += reward\n",
    "        self.total_q_max += np.max(self.q_values.eval(feed_dict={self.s: [np.float32(state / 255.0)]}))\n",
    "        self.duration += 1\n",
    "\n",
    "        if terminal:\n",
    "            # Write summary\n",
    "            if self.t >= INITIAL_REPLAY_SIZE:\n",
    "                stats = [self.total_reward, self.total_q_max / float(self.duration),\n",
    "                        self.duration, self.total_loss / (float(self.duration) / float(TRAIN_INTERVAL))]\n",
    "                for i in range(len(stats)):\n",
    "                    self.sess.run(self.update_ops[i], feed_dict={\n",
    "                        self.summary_placeholders[i]: float(stats[i])\n",
    "                    })\n",
    "                summary_str = self.sess.run(self.summary_op)\n",
    "                self.summary_writer.add_summary(summary_str, self.episode + 1)\n",
    "\n",
    "            # Debug\n",
    "            if self.t < INITIAL_REPLAY_SIZE:\n",
    "                mode = 'random'\n",
    "            elif INITIAL_REPLAY_SIZE <= self.t < INITIAL_REPLAY_SIZE + EXPLORATION_STEPS:\n",
    "                mode = 'explore'\n",
    "            else:\n",
    "                mode = 'exploit'\n",
    "            print('EPISODE: {0:6d} / TIMESTEP: {1:8d} / DURATION: {2:5d} / EPSILON: {3:.5f} / TOTAL_REWARD: {4:3.0f} / AVG_MAX_Q: {5:2.4f} / AVG_LOSS: {6:.5f} / MODE: {7}'.format(\n",
    "                self.episode + 1, self.t, self.duration, self.epsilon,\n",
    "                self.total_reward, self.total_q_max / float(self.duration),\n",
    "                self.total_loss / (float(self.duration) / float(TRAIN_INTERVAL)), mode))\n",
    "            with open(\"BreakoutgameDQN.txt\", \"a\") as f:\n",
    "                f.write(\"Simulation {}: Total reward {}  total loss {}\\n\".format(str(self.episode + 1), self.total_reward, self.total_reward))\n",
    "\n",
    "            self.total_reward = 0\n",
    "            self.total_q_max = 0\n",
    "            self.total_loss = 0\n",
    "            self.duration = 0\n",
    "            self.episode += 1\n",
    "\n",
    "        self.t += 1\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def train_network(self):\n",
    "        state_batch = []\n",
    "        action_batch = []\n",
    "        reward_batch = []\n",
    "        next_state_batch = []\n",
    "        terminal_batch = []\n",
    "        y_batch = []\n",
    "\n",
    "        # Sample random minibatch of transition from replay memory\n",
    "        minibatch = random.sample(self.replay_memory, BATCH_SIZE)\n",
    "        for data in minibatch:\n",
    "            state_batch.append(data[0])\n",
    "            action_batch.append(data[1])\n",
    "            reward_batch.append(data[2])\n",
    "            next_state_batch.append(data[3])\n",
    "            terminal_batch.append(data[4])\n",
    "\n",
    "        # Convert True to 1, False to 0\n",
    "        terminal_batch = np.array(terminal_batch) + 0\n",
    "\n",
    "        target_q_values_batch = self.target_q_values.eval(feed_dict={self.st: np.float32(np.array(next_state_batch) / 255.0)})\n",
    "        y_batch = reward_batch + (1 - terminal_batch) * GAMMA * np.max(target_q_values_batch, axis=1)\n",
    "\n",
    "        loss, _ = self.sess.run([self.loss, self.grads_update], feed_dict={\n",
    "            self.s: np.float32(np.array(state_batch) / 255.0),\n",
    "            self.a: action_batch,\n",
    "            self.y: y_batch\n",
    "        })\n",
    "\n",
    "        self.total_loss += loss\n",
    "        \n",
    "        \n",
    "    def setup_summary(self):\n",
    "        episode_total_reward = tf.Variable(0.)\n",
    "        tf.summary.scalar(ENV_NAME + '/Total Reward/Episode', episode_total_reward)\n",
    "        episode_avg_max_q = tf.Variable(0.)\n",
    "        tf.summary.scalar(ENV_NAME + '/Average Max Q/Episode', episode_avg_max_q)\n",
    "        episode_duration = tf.Variable(0.)\n",
    "        tf.summary.scalar(ENV_NAME + '/Duration/Episode', episode_duration)\n",
    "        episode_avg_loss = tf.Variable(0.)\n",
    "        tf.summary.scalar(ENV_NAME + '/Average Loss/Episode', episode_avg_loss)\n",
    "        summary_vars = [episode_total_reward, episode_avg_max_q, episode_duration, episode_avg_loss]\n",
    "        summary_placeholders = [tf.placeholder(tf.float32) for _ in range(len(summary_vars))]\n",
    "        update_ops = [summary_vars[i].assign(summary_placeholders[i]) for i in range(len(summary_vars))]\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        return summary_placeholders, update_ops, summary_op\n",
    "\n",
    "    def load_network(self):\n",
    "        checkpoint = tf.train.get_checkpoint_state(SAVE_NETWORK_PATH)\n",
    "        if checkpoint and checkpoint.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, checkpoint.model_checkpoint_path)\n",
    "            print('Successfully loaded: ' + checkpoint.model_checkpoint_path)\n",
    "        else:\n",
    "            print('Training new network...')\n",
    "\n",
    "    def get_action_at_test(self, state):\n",
    "        if random.random() <= 0.05:\n",
    "            action = random.randrange(self.num_actions)\n",
    "        else:\n",
    "            action = np.argmax(self.q_values.eval(feed_dict={self.s: [np.float32(state / 255.0)]}))\n",
    "\n",
    "        self.t += 1\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
