{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "import tensorflow\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, Flatten, Dense\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_image_data_format('channels_first')\n",
    "\n",
    "BATCH_SIZE = 32  # Mini batch size\n",
    "LOAD_NETWORK = False\n",
    "TRAIN = True\n",
    "SAVE_INTERVAL = 1000  # The frequency with which the network is saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_WIDTH = 84  # Resized frame width\n",
    "FRAME_HEIGHT = 84  # Resized frame height\n",
    "class Agent():\n",
    "    \n",
    "    def __init__(self, num_actions):\n",
    "        self.ENV_NAME = 'Breakout-v0'  # Game and its version\n",
    "        self.SAVE_NETWORK_PATH = 'saved_networks/' + self.ENV_NAME\n",
    "        self.SAVE_SUMMARY_PATH = 'summary/' + self.ENV_NAME\n",
    "        self.EXPLORATION_STEPS = 1000000  # Steps for linearly decreasing epsilon\n",
    "        self.GAMMA = 0.99  # Discount factor\n",
    "        self.INITIAL_EPSILON = 1.0  # Initial value of epsilon in epsilon-greedy\n",
    "        self.FINAL_EPSILON = 0.1  # Final value of epsilon in epsilon-greedy\n",
    "        self.TRAIN_INTERVAL = 4  # The agent selects 4 actions between successive updates\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.epsilon = self.INITIAL_EPSILON\n",
    "        self.epsilon_step = (self.INITIAL_EPSILON - self.FINAL_EPSILON) / self.EXPLORATION_STEPS\n",
    "        self.t = 0\n",
    "        \n",
    "        # Parameters used for summary\n",
    "        self.total_reward = 0\n",
    "        self.total_q_max = 0\n",
    "        self.total_loss = 0\n",
    "        self.duration = 0\n",
    "        self.episode = 0\n",
    "\n",
    "        # Create replay memory\n",
    "        self.replay_memory = deque()\n",
    "\n",
    "        # Create q network\n",
    "        self.s, self.q_values, q_network = self.build_model()\n",
    "        q_network_weights = q_network.trainable_weights\n",
    "        \n",
    "        # Create target network\n",
    "        self.st, self.target_q_values, target_network = self.build_model()\n",
    "        target_network_weights = target_network.trainable_weights\n",
    "\n",
    "        # Define target network update operation\n",
    "        self.update_target_network = [target_network_weights[i].assign(q_network_weights[i]) for i in range(len(target_network_weights))]\n",
    "\n",
    "        # Define loss and gradient update operation\n",
    "        self.a, self.y, self.loss, self.grads_update = self.build_training_op(q_network_weights)\n",
    "\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.saver = tf.train.Saver(q_network_weights)\n",
    "\n",
    "        if not os.path.exists(self.SAVE_NETWORK_PATH):\n",
    "            os.makedirs(self.SAVE_NETWORK_PATH)\n",
    "\n",
    "        self.sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        # Load network\n",
    "        if LOAD_NETWORK:\n",
    "            self.load()\n",
    "\n",
    "        # Initialize target network\n",
    "        self.sess.run(self.update_target_network)\n",
    "        \n",
    "   \n",
    "    STATE_LENGTH = 4  # Number of most recent frames to produce the input to the network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Convolution2D(32, 8, 8, subsample=(4, 4), activation='relu', input_shape=(self.STATE_LENGTH, FRAME_WIDTH, FRAME_HEIGHT)))\n",
    "        model.add(Convolution2D(64, 4, 4, subsample=(2, 2), activation='relu'))\n",
    "        model.add(Convolution2D(64, 3, 3, subsample=(1, 1), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(self.num_actions))\n",
    "\n",
    "        s = tf.placeholder(tf.float32, [None, self.STATE_LENGTH, FRAME_WIDTH, FRAME_HEIGHT])\n",
    "        q_values = model(s)\n",
    "\n",
    "        return s, q_values, model\n",
    "\n",
    "    def build_training_op(self, q_network_weights):\n",
    "        a = tf.placeholder(tf.int64, [None])\n",
    "        y = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "        # Convert action to one hot vector\n",
    "        a_one_hot = tf.one_hot(a, self.num_actions, 1.0, 0.0)\n",
    "        q_value = tf.reduce_sum(tf.multiply(self.q_values, a_one_hot), reduction_indices=1)\n",
    "\n",
    "        # Clip the error, the loss is quadratic when the error is in (-1, 1), and linear outside of that region\n",
    "        error = tf.abs(y - q_value)\n",
    "        quadratic_part = tf.clip_by_value(error, 0.0, 1.0)\n",
    "        linear_part = error - quadratic_part\n",
    "        loss = tf.reduce_mean(0.5 * tf.square(quadratic_part) + linear_part)\n",
    "        \n",
    "        MOMENTUM = 0.95  \n",
    "        LEARNING_RATE = 0.00025  # Learning rate used by RMSProp\n",
    "        MIN_GRAD = 0.01  # Constant added to the squared gradient in the denominator of the RMSProp update\n",
    "        optimizer = tf.train.RMSPropOptimizer(LEARNING_RATE, momentum=MOMENTUM, epsilon=MIN_GRAD)\n",
    "        grads_update = optimizer.minimize(loss, var_list=q_network_weights)\n",
    "\n",
    "        return a, y, loss, grads_update\n",
    "\n",
    "  \n",
    "    INITIAL_REPLAY_SIZE = 20000  # Number of steps to populate the replay memory before training starts\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        if self.epsilon >= random.random() or self.t < self.INITIAL_REPLAY_SIZE:\n",
    "            action = random.randrange(self.num_actions)\n",
    "        else:\n",
    "            action = np.argmax(self.q_values.eval(feed_dict={self.s: [np.float32(state / 255.0)]}))\n",
    "\n",
    "        # Decreasing espsilon over time\n",
    "        if self.epsilon > self.FINAL_EPSILON and self.t >= self.INITIAL_REPLAY_SIZE:\n",
    "            self.epsilon -= self.epsilon_step\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def remember(self,state, action, reward, next_state, terminal):\n",
    "        self.replay_memory.append((state, action, reward, next_state, terminal))\n",
    "    \n",
    "    def act(self, state, action, reward, terminal, observation):\n",
    "        next_state = np.append(state[1:, :, :], observation, axis=0)\n",
    "\n",
    "        # Clip all positive rewards at 1 and all negative rewards at -1, leaving 0 rewards unchanged\n",
    "        reward = np.clip(reward, -1, 1)\n",
    "\n",
    "        # Store transition in replay memory\n",
    "        NUM_REPLAY_MEMORY = 400000  # Number of replay memory the agent uses for training\n",
    "        \n",
    "        self.remember(state, action, reward, next_state, terminal)\n",
    "        \n",
    "        if len(self.replay_memory) > NUM_REPLAY_MEMORY:\n",
    "            self.replay_memory.popleft()\n",
    "\n",
    "        TARGET_UPDATE_INTERVAL = 10000  # The frequency with which the target network is updated\n",
    "        if self.t >= self.INITIAL_REPLAY_SIZE:\n",
    "            # Train network\n",
    "            if self.t % self.TRAIN_INTERVAL == 0:\n",
    "                self.replay()\n",
    "\n",
    "            # Update target network\n",
    "            if self.t % TARGET_UPDATE_INTERVAL == 0:\n",
    "                self.sess.run(self.update_target_network)\n",
    "\n",
    "            # Save network\n",
    "            if self.t % SAVE_INTERVAL == 0:\n",
    "                self.save()\n",
    "\n",
    "        self.total_reward += reward\n",
    "        self.total_q_max += np.max(self.q_values.eval(feed_dict={self.s: [np.float32(state / 255.0)]}))\n",
    "        self.duration += 1\n",
    "\n",
    "        if terminal:\n",
    "            # Debug\n",
    "            if self.t < self.INITIAL_REPLAY_SIZE:\n",
    "                mode = 'random'\n",
    "            elif self.INITIAL_REPLAY_SIZE <= self.t < self.INITIAL_REPLAY_SIZE + self.EXPLORATION_STEPS:\n",
    "                mode = 'explore'\n",
    "            else:\n",
    "                mode = 'exploit'\n",
    "            print('EPISODE: {0:6d} / TIMESTEP: {1:8d} / DURATION: {2:5d} / EPSILON: {3:.5f} / TOTAL_REWARD: {4:3.0f} / AVG_MAX_Q: {5:2.4f} / AVG_LOSS: {6:.5f} / MODE: {7}'.format(\n",
    "                self.episode + 1, self.t, self.duration, self.epsilon,\n",
    "                self.total_reward, self.total_q_max / float(self.duration),\n",
    "                self.total_loss / (float(self.duration) / float(self.TRAIN_INTERVAL)), mode))\n",
    "            with open(\"BreakoutgameDQN.txt\", \"a\") as f:\n",
    "                f.write(\"Simulation {}: Total reward {}  total loss {}\\n\".format(str(self.episode + 1), self.total_reward, self.total_reward))\n",
    "\n",
    "            self.total_reward = 0\n",
    "            self.total_q_max = 0\n",
    "            self.total_loss = 0\n",
    "            self.duration = 0\n",
    "            self.episode += 1\n",
    "\n",
    "        self.t += 1\n",
    "\n",
    "        return next_state\n",
    "    \n",
    "    #funtion for training network\n",
    "    def replay(self):\n",
    "        state_batch = []\n",
    "        action_batch = []\n",
    "        reward_batch = []\n",
    "        next_state_batch = []\n",
    "        terminal_batch = []\n",
    "        y_batch = []\n",
    "\n",
    "        # Sample random minibatch of transition from replay memory\n",
    "        minibatch = random.sample(self.replay_memory, BATCH_SIZE)\n",
    "        for data in minibatch:\n",
    "            state_batch.append(data[0])\n",
    "            action_batch.append(data[1])\n",
    "            reward_batch.append(data[2])\n",
    "            next_state_batch.append(data[3])\n",
    "            terminal_batch.append(data[4])\n",
    "\n",
    "        # Convert True to 1, False to 0\n",
    "        terminal_batch = np.array(terminal_batch) + 0\n",
    "\n",
    "        target_q_values_batch = self.target_q_values.eval(feed_dict={self.st: np.float32(np.array(next_state_batch) / 255.0)})\n",
    "        y_batch = reward_batch + (1 - terminal_batch) * self.GAMMA * np.max(target_q_values_batch, axis=1)\n",
    "\n",
    "        loss, _ = self.sess.run([self.loss, self.grads_update], feed_dict={\n",
    "            self.s: np.float32(np.array(state_batch) / 255.0),\n",
    "            self.a: action_batch,\n",
    "            self.y: y_batch\n",
    "        })\n",
    "\n",
    "        self.total_loss += loss\n",
    "        \n",
    "    def load(self):\n",
    "        checkpoint = tf.train.get_checkpoint_state(self.SAVE_NETWORK_PATH)\n",
    "        if checkpoint and checkpoint.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, checkpoint.model_checkpoint_path)\n",
    "            \n",
    "    def save(self):\n",
    "        save_path = self.saver.save(self.sess, self.SAVE_NETWORK_PATH + '/' + self.ENV_NAME, global_step=self.t)\n",
    "        print('Successfully saved: ' + save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarmad Saeed\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:63: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), activation=\"relu\", input_shape=(4, 84, 84..., strides=(4, 4))`\n",
      "C:\\Users\\Sarmad Saeed\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:64: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (4, 4), activation=\"relu\", strides=(2, 2))`\n",
      "C:\\Users\\Sarmad Saeed\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:65: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", strides=(1, 1))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sarmad Saeed\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\Sarmad Saeed\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Sarmad Saeed\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\tf_should_use.py:198: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "EPISODE:      1 / TIMESTEP:      233 / DURATION:   234 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0497 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:      2 / TIMESTEP:      400 / DURATION:   167 / EPSILON: 1.00000 / TOTAL_REWARD:   0 / AVG_MAX_Q: 0.0484 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:      3 / TIMESTEP:      641 / DURATION:   241 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0493 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:      4 / TIMESTEP:      927 / DURATION:   286 / EPSILON: 1.00000 / TOTAL_REWARD:   2 / AVG_MAX_Q: 0.0465 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:      5 / TIMESTEP:     1095 / DURATION:   168 / EPSILON: 1.00000 / TOTAL_REWARD:   0 / AVG_MAX_Q: 0.0474 / AVG_LOSS: 0.00000 / MODE: random\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-618306c59bef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[1;31m# doing pre processing here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0mprocessed_observation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_observation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[0mprocessed_observation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrgb2gray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_observation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mFRAME_WIDTH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFRAME_HEIGHT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m                 \u001b[0mprocessed_observation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_observation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFRAME_WIDTH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFRAME_HEIGHT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(image, output_shape, order, mode, cval, clip, preserve_range, anti_aliasing, anti_aliasing_sigma)\u001b[0m\n\u001b[0;32m    177\u001b[0m         out = warp(image, tform, output_shape=output_shape, order=order,\n\u001b[0;32m    178\u001b[0m                    \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m                    preserve_range=preserve_range)\n\u001b[0m\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# n-dimensional interpolation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py\u001b[0m in \u001b[0;36mwarp\u001b[1;34m(image, inverse_map, map_args, output_shape, order, mode, cval, clip, preserve_range)\u001b[0m\n\u001b[0;32m    863\u001b[0m                 warped = _warp_fast(image, matrix,\n\u001b[0;32m    864\u001b[0m                                     \u001b[0moutput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 865\u001b[1;33m                                     order=order, mode=mode, cval=cval)\n\u001b[0m\u001b[0;32m    866\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m                 \u001b[0mdims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mskimage\\transform\\_warps_cy.pyx\u001b[0m in \u001b[0;36mskimage.transform._warps_cy._warp_fast\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m \u001b[1;33m@\u001b[0m\u001b[0mset_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'numpy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    470\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m     \"\"\"Convert the input to an array.\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('Breakout-v0')\n",
    "    agent = Agent(num_actions=env.action_space.n)\n",
    "    \n",
    "    EPISODES = 30000  # times the game is played\n",
    "    NO_OP_STEPS = 30  # Maximum number of \"do nothing\" actions to be performed by the agent at the start of an episode\n",
    "    STATE_LENGTH= 4\n",
    "    \n",
    "    if TRAIN:  # Train mode\n",
    "        for _ in range(EPISODES):\n",
    "            terminal = False\n",
    "            observation = env.reset()\n",
    "            for _ in range(random.randint(1, NO_OP_STEPS)):\n",
    "                last_observation = observation\n",
    "                observation, _, _, _ = env.step(0)  # Do nothing\n",
    "            processed_observation = np.maximum(observation, last_observation)\n",
    "            processed_observation = np.uint8(resize(rgb2gray(processed_observation), (FRAME_WIDTH, FRAME_HEIGHT)) * 255)\n",
    "            state = [processed_observation for _ in range(STATE_LENGTH)]\n",
    "            state =  np.stack(state, axis=0)\n",
    "            #state = agent.get_initial_state(observation, last_observation)\n",
    "            while not terminal:\n",
    "                last_observation = observation\n",
    "                action = agent.get_action(state)\n",
    "                observation, reward, terminal, _ = env.step(action)\n",
    "                \n",
    "                # doing pre processing here                \n",
    "                processed_observation = np.maximum(observation, last_observation)\n",
    "                processed_observation = np.uint8(resize(rgb2gray(processed_observation), (FRAME_WIDTH, FRAME_HEIGHT)) * 255)\n",
    "                processed_observation = np.reshape(processed_observation, (1, FRAME_WIDTH, FRAME_HEIGHT))\n",
    "                \n",
    "                state = agent.act(state, action, reward, terminal, processed_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
